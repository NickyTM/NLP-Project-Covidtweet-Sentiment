{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import preprocessor.api as p\n",
    "from contractions import contractions_dict\n",
    "from nltk.corpus import wordnet\n",
    "import pattern\n",
    "from pattern.en import suggest, lemma\n",
    "import wordninja\n",
    "from numpy import nan\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import nltk\n",
    "from stopword import stopwords\n",
    "from nltk.corpus import words\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from spellchecker import SpellChecker\n",
    "from ast import literal_eval\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessFeatures(data):\n",
    "    data[\"Hashtags\"] = data[\"OriginalTweet\"].apply(lambda x: [', '.join(map(str, re.findall(r\"#(\\w+)\", x))).lower()])\n",
    "    data[\"HashtagsCount\"] = data[\"OriginalTweet\"].apply(lambda x: [len(re.findall(r\"#(\\w+)\", x))])\n",
    "    data[\"Mentions\"] = data[\"OriginalTweet\"].apply(lambda x: [len(re.findall(r\"@(\\w+)\", x))])\n",
    "    data[\"URLs\"] = data[\"OriginalTweet\"].apply(lambda x: [len(re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', x))])\n",
    "    data[\"UpperCaseWords\"] = data[\"OriginalTweet\"].apply(lambda x: [len(re.findall(r\"(\\b[A-Z][A-Z]+|\\b[A-Z]\\b)\", x))])\n",
    "    data[\"PunctPeriod\"] = data[\"OriginalTweet\"].apply(lambda x: [len(re.findall(r\"\\.\", x))])\n",
    "    data[\"PunctExclamation\"] = data[\"OriginalTweet\"].apply(lambda x: [len(re.findall(r\"\\!\", x))])\n",
    "    data[\"PunctQuestion\"] = data[\"OriginalTweet\"].apply(lambda x: [len(re.findall(r\"\\?\", x))])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec and general features\n",
    "Predicting sentiment for tweets based on word2vec model and general features.\n",
    "\n",
    "## Prepare trainingset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/preprocess_train.csv\", encoding = 'latin1') \n",
    "data = data.dropna()\n",
    "data['tokens'] = data['tokens'].apply(literal_eval)\n",
    "\n",
    "corpus = []\n",
    "for item in data['tokens']:\n",
    "    corpus.append(' '.join(item))\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format('../MLworkshop Jeopardy/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=10 ** 5)\n",
    "#build tf-idf vector\n",
    "tfidf = TfidfVectorizer(min_df=3)\n",
    "tfidf.fit(corpus)\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "def get_ifidf_for_words(text):\n",
    "    tfidf_matrix= tfidf.transform([text]).todense()\n",
    "    feature_index = tfidf_matrix[0,:].nonzero()[1]\n",
    "    tfidf_scores = zip([feature_names[i] for i in feature_index], [tfidf_matrix[0, x] for x in feature_index])\n",
    "    return dict(tfidf_scores)\n",
    "\n",
    "def w2vmean(l):\n",
    "    text = ' '.join(l)\n",
    "    tf_idf = get_ifidf_for_words(text)\n",
    "    X1 = np.zeros((300,))\n",
    "    for x in l:\n",
    "        if x in word2vec and x in tf_idf:\n",
    "            X1 += word2vec[x] * tf_idf[x]\n",
    "    return X1\n",
    "\n",
    "data['tokens'] = data['tokens'].apply(lambda x: w2vmean(x))\n",
    "data = preprocessFeatures(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>HashtagsCount</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>URLs</th>\n",
       "      <th>UpperCaseWords</th>\n",
       "      <th>PunctPeriod</th>\n",
       "      <th>PunctExclamation</th>\n",
       "      <th>PunctQuestion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[-0.045217821723781526, -0.12957120189093985, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0.1090496089309454, 0.11484042098163627, 0.10...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0.18580312211997807, -0.07340790703892708, 0....</td>\n",
       "      <td>[covid19france, covid_19, covid19, coronavirus...</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>[0.27922972617670894, -0.03418381605297327, 0....</td>\n",
       "      <td>[covid19, coronavirus, coronavirusfrance, rest...</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As news of the regionÃÂs first confirmed COV...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0.04783004254568368, 0.059689379995688796, -0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "5  As news of the regionÃÂs first confirmed COV...            Positive   \n",
       "\n",
       "                                              tokens  \\\n",
       "1  [-0.045217821723781526, -0.12957120189093985, ...   \n",
       "2  [0.1090496089309454, 0.11484042098163627, 0.10...   \n",
       "3  [0.18580312211997807, -0.07340790703892708, 0....   \n",
       "4  [0.27922972617670894, -0.03418381605297327, 0....   \n",
       "5  [0.04783004254568368, 0.059689379995688796, -0...   \n",
       "\n",
       "                                            Hashtags HashtagsCount Mentions  \\\n",
       "1                                                 []           [0]      [0]   \n",
       "2                                                 []           [0]      [0]   \n",
       "3  [covid19france, covid_19, covid19, coronavirus...           [7]      [0]   \n",
       "4  [covid19, coronavirus, coronavirusfrance, rest...           [6]      [0]   \n",
       "5                                                 []           [0]      [1]   \n",
       "\n",
       "  URLs UpperCaseWords PunctPeriod PunctExclamation PunctQuestion  \n",
       "1  [0]            [1]         [0]              [0]           [0]  \n",
       "2  [1]            [1]         [1]              [0]           [0]  \n",
       "3  [1]           [11]         [6]              [0]           [0]  \n",
       "4  [1]            [2]         [7]              [0]           [0]  \n",
       "5  [1]            [1]         [1]              [0]           [0]  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04521782 -0.1295712  -0.03599979 -0.03202558 -0.13110606 -0.0385706\n",
      "  0.06007127 -0.19903495  0.48167129  0.26953838 -0.06201406 -0.1218441\n",
      " -0.13611413  0.12963183 -0.67325786  0.39216037  0.19548158  0.2820546\n",
      " -0.01657487 -0.2635157   0.54904745  0.11781592 -0.13391594  0.15330124\n",
      " -0.01461942 -0.07108847 -0.5629061   0.31965716  0.09751459 -0.02628797\n",
      " -0.19790317 -0.36990661 -0.30469168 -0.36872784 -0.28543786 -0.28384049\n",
      "  0.35927944  0.00246197 -0.04935208  0.29575096  0.02854061 -0.03263297\n",
      "  0.06597441 -0.14169426 -0.29342194 -0.32867927 -0.23560393  0.13801583\n",
      " -0.35742032 -0.11407956 -0.14610504 -0.12540382 -0.05014944 -0.14147434\n",
      " -0.03050272 -0.02898542 -0.09951725 -0.15503716 -0.16825065 -0.12854678\n",
      " -0.17628692 -0.17272498 -0.31981235  0.11415113 -0.0162946   0.37618858\n",
      " -0.54127505  0.42038736  0.02666528  0.2927442   0.03846314  0.01600092\n",
      "  0.63625876  0.11625102 -0.58021663  0.05137449  0.23431435  0.44318428\n",
      "  0.24013518 -0.03901059  0.23346466 -0.1976378   0.10083642  0.20689596\n",
      "  0.01789333 -0.23957685 -0.46436597  0.17938392 -0.08316787  0.27076973\n",
      "  0.27434829  0.24748793 -0.63954813 -0.88018598  0.09924747  0.01122061\n",
      " -0.0195714   0.06311035  0.19784781 -0.26661193 -0.23380754 -0.49815954\n",
      " -0.11647066  0.39479216  0.09278085 -0.47483247 -0.0633423  -0.07260747\n",
      "  0.23217077  0.00867231 -0.08341036 -0.42505055 -0.30743019 -0.53680941\n",
      "  0.15305104  0.09011814 -0.10241419 -0.16243587  0.86218051  0.46957517\n",
      " -0.49070517  0.39900377 -0.31497765  0.66911347  0.28296031 -0.10182241\n",
      " -0.23099909 -0.45089992  0.29533373  0.43318469 -0.03588073 -0.63643939\n",
      " -0.40879061 -0.14733592 -0.09345967 -0.23306594 -0.16821778  0.15804167\n",
      " -0.08156015  0.12693961  0.1836596  -0.10422981 -0.0388713   0.32125224\n",
      "  0.25665321  0.31603581 -0.1081661  -0.39758499 -0.47277077  0.23657194\n",
      "  0.74027125  0.44731292 -0.3043724   0.11344478  0.09686933 -0.49851309\n",
      " -0.25370177 -0.30852374 -0.26901762 -0.09308902 -0.40207565  0.46156818\n",
      "  0.03970939 -0.08548015 -0.02453746 -0.00720868 -0.07671003 -0.51956227\n",
      " -0.06296346 -0.3161487  -0.25838603  0.1111845  -0.09656814 -0.67387173\n",
      " -0.58063486 -0.2565327   0.47059963 -0.16118343 -0.46456896  0.24986595\n",
      " -0.36695098 -0.24112469  0.05514729  0.11592064  0.51382068 -0.45615761\n",
      " -0.1694997   0.40589321 -0.07978148  0.39654329 -0.28126597 -0.27520746\n",
      " -0.16621179  0.15907976 -0.16161929  0.23605652 -0.22176851 -0.17132468\n",
      " -0.13831694 -0.60460293  0.0111721   0.75983437  0.05226977 -0.28949825\n",
      " -0.10208201 -0.05890625 -0.27025153 -0.0496062   0.11205503 -0.10305065\n",
      "  0.01735331 -0.05367871 -0.2628164  -0.24064734 -0.15607197  0.39957548\n",
      "  0.27444684  0.18559163 -0.75526157  0.07229714  0.08276545  0.25939163\n",
      " -0.17208243  0.00471817  0.2903035   0.03798967 -0.14776344  0.18684402\n",
      " -0.17721193  0.0035297   0.23957992 -0.07856107 -0.13986394 -0.23984607\n",
      "  0.26422493 -0.45635445  0.28708533 -0.28688613 -0.44610652 -0.02673019\n",
      "  0.24526417 -0.0660641  -0.0285436  -0.41011817  0.49996792  0.31485855\n",
      "  0.12440597  0.16378731  0.04679703 -0.35473429 -0.12670339  0.13711185\n",
      "  0.21139856  0.13698987  0.25280809  0.05945187  0.27020388 -0.34051124\n",
      " -0.43940976  0.10658127  0.07468064 -0.23133763 -0.6542546  -0.13603362\n",
      "  0.03741821  0.7038952  -0.1550848   0.0192078  -0.46533787 -0.23540067\n",
      "  0.29035959  0.36398034  0.4583884  -0.05488196  0.11860827 -0.35857842\n",
      "  0.16020646 -0.59284296  0.19115831 -0.09873478  0.15718342 -0.03712953\n",
      "  0.19221001  0.16846936 -0.17116538  0.01517593 -0.23723344  0.37393441\n",
      "  0.32823932  0.45325934 -0.02711843  0.45807185 -0.27584718 -0.20166472\n",
      "  0.21363927 -0.0688255  -0.21754054 -0.10169286 -0.18683029 -0.30936813\n",
      "  0.          0.          0.          1.          0.          0.\n",
      "  0.        ]\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = [] \n",
    "\n",
    "for i in range(len(data)):\n",
    "    tok = data['tokens'].iloc[i]\n",
    "    has = data['HashtagsCount'].iloc[i]\n",
    "    men = data['Mentions'].iloc[i]\n",
    "    url = data['URLs'].iloc[i]\n",
    "    upc = data['UpperCaseWords'].iloc[i]\n",
    "    per = data['PunctPeriod'].iloc[i]\n",
    "    exc = data['PunctExclamation'].iloc[i]\n",
    "    que = data['PunctQuestion'].iloc[i]\n",
    "    row = np.concatenate([tok, has, men, url, upc, per, exc, que])\n",
    "    X.append(row)\n",
    "    y.append(data['Sentiment'].iloc[i])\n",
    "    \n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pd.read_csv(\"data/preprocess_test_final.csv\", encoding = 'latin1') \n",
    "testdata = testdata.dropna()\n",
    "testdata['tokens'] = testdata['tokens'].apply(literal_eval)\n",
    "testdata['tokens'] = testdata['tokens'].apply(lambda x: w2vmean(x))\n",
    "testdata = preprocessFeatures(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sent_token</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>HashtagsCount</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>URLs</th>\n",
       "      <th>UpperCaseWords</th>\n",
       "      <th>PunctPeriod</th>\n",
       "      <th>PunctExclamation</th>\n",
       "      <th>PunctQuestion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>['TRENDING: New Yorkers encounter empty superm...</td>\n",
       "      <td>[0.1975125118624419, 0.16406913893297315, -0.0...</td>\n",
       "      <td>[coronavirus]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[\"When I couldn't find hand sanitizer at Fred ...</td>\n",
       "      <td>[0.012639568885788321, 0.09953883814159781, -0...</td>\n",
       "      <td>[amazon, coronavirus]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>['Find out how you can protect yourself and lo...</td>\n",
       "      <td>[-0.01927915937267244, -0.0742998868227005, -0...</td>\n",
       "      <td>[coronavirus]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>['#Panic buying hits #NewYork City as anxious ...</td>\n",
       "      <td>[-0.16155412141233683, 0.04214186384342611, -0...</td>\n",
       "      <td>[panic, newyork, healthcare, bigapple, coronav...</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>['#toiletpaper #dunnypaper #coronavirus #coron...</td>\n",
       "      <td>[0.13522900408133864, -0.1849645283073187, -0....</td>\n",
       "      <td>[toiletpaper, dunnypaper, coronavirus, coronav...</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral   \n",
       "\n",
       "                                          sent_token  \\\n",
       "0  ['TRENDING: New Yorkers encounter empty superm...   \n",
       "1  [\"When I couldn't find hand sanitizer at Fred ...   \n",
       "2  ['Find out how you can protect yourself and lo...   \n",
       "3  ['#Panic buying hits #NewYork City as anxious ...   \n",
       "4  ['#toiletpaper #dunnypaper #coronavirus #coron...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [0.1975125118624419, 0.16406913893297315, -0.0...   \n",
       "1  [0.012639568885788321, 0.09953883814159781, -0...   \n",
       "2  [-0.01927915937267244, -0.0742998868227005, -0...   \n",
       "3  [-0.16155412141233683, 0.04214186384342611, -0...   \n",
       "4  [0.13522900408133864, -0.1849645283073187, -0....   \n",
       "\n",
       "                                            Hashtags HashtagsCount Mentions  \\\n",
       "0                                      [coronavirus]           [1]      [0]   \n",
       "1                              [amazon, coronavirus]           [2]      [0]   \n",
       "2                                      [coronavirus]           [1]      [0]   \n",
       "3  [panic, newyork, healthcare, bigapple, coronav...          [11]      [0]   \n",
       "4  [toiletpaper, dunnypaper, coronavirus, coronav...          [11]      [0]   \n",
       "\n",
       "  URLs UpperCaseWords PunctPeriod PunctExclamation PunctQuestion  \n",
       "0  [2]            [1]         [2]              [0]           [0]  \n",
       "1  [1]            [2]         [4]              [2]           [2]  \n",
       "2  [0]            [0]         [1]              [0]           [1]  \n",
       "3  [2]            [6]         [2]              [0]           [1]  \n",
       "4  [1]            [0]         [2]              [0]           [0]  "
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19751251  0.16406914 -0.08077186  0.24873493 -0.2236984  -0.02667778\n",
      "  0.03970695 -0.09416564  0.15056637  0.49147954 -0.18753959 -0.05954439\n",
      " -0.076067    0.14337692 -0.5960996   0.31317267 -0.04269212  0.1886496\n",
      "  0.0173678  -0.46386353  0.24728703  0.14169849 -0.01813464 -0.04459166\n",
      " -0.05791089  0.01703206 -0.01445083  0.08936925  0.21961409 -0.13089258\n",
      " -0.02144561  0.07451568 -0.25947715 -0.14026356 -0.05995768  0.08417742\n",
      "  0.14595186 -0.1311665   0.17901262  0.10232114  0.14483824  0.17584943\n",
      "  0.21348609  0.03781949 -0.2827316  -0.25911544 -0.06880158  0.05365289\n",
      " -0.17094703  0.05027327  0.07664106  0.14587476 -0.19888585 -0.12783226\n",
      "  0.12424986 -0.19563302  0.00774756 -0.18017244  0.2547958  -0.08037268\n",
      " -0.10337515  0.01341996 -0.24838854  0.14635911 -0.30849758  0.02157698\n",
      " -0.10823765  0.15720974  0.10605738  0.12602015 -0.07108232  0.17590671\n",
      "  0.21743952  0.20612701 -0.39072273  0.02816121  0.13147974  0.32887982\n",
      "  0.08348509  0.17350902  0.0487698  -0.27896093  0.17237388 -0.13369453\n",
      "  0.13272205  0.15422608 -0.24198799  0.19205977  0.10457    -0.03660005\n",
      "  0.13339865 -0.09125687 -0.29298055  0.04625358 -0.04584474 -0.05216529\n",
      "  0.16911994  0.17774936  0.09093266 -0.0023599   0.15440793 -0.06085693\n",
      " -0.05258558 -0.0653673   0.23149419 -0.42805516 -0.0426106  -0.03771289\n",
      " -0.02413051 -0.08053472  0.08541561 -0.06058884  0.00738425 -0.2143093\n",
      "  0.11120974 -0.25393374  0.06485401 -0.06623021  0.26758571  0.27653802\n",
      " -0.05931437 -0.14276723 -0.22198145  0.09963645 -0.06523434  0.0688066\n",
      " -0.09492301 -0.09303237  0.08248964  0.22418382 -0.09815485 -0.14735984\n",
      "  0.00652888  0.16382311  0.03793776  0.12631357 -0.08141186  0.16454645\n",
      " -0.34697858  0.0804178   0.27003656 -0.47439067 -0.12682074  0.0624681\n",
      "  0.15987233 -0.10732967  0.03226423 -0.18852333 -0.2734011  -0.10346964\n",
      "  0.06943845  0.00211249 -0.20207323  0.12108171 -0.09144524 -0.10757578\n",
      " -0.24550526 -0.45698932 -0.23086602  0.03548447 -0.14320574  0.0230671\n",
      " -0.06327092 -0.24045715 -0.02304434 -0.03270765  0.3242679  -0.12888756\n",
      " -0.01969518 -0.06863271 -0.19618603 -0.18552941 -0.2321452  -0.22535587\n",
      " -0.20863198  0.09735194  0.19544132 -0.09523545  0.01789872  0.10417852\n",
      " -0.35836347 -0.31362822  0.04743513 -0.32232821  0.11672675 -0.26700878\n",
      " -0.09853852  0.42764071  0.08689826  0.38295452 -0.00469036 -0.06439551\n",
      " -0.07377241  0.13441635 -0.24590511  0.17260523  0.14328456  0.28830447\n",
      " -0.21900562 -0.0446525   0.04514054  0.12026611 -0.21141133 -0.28553823\n",
      "  0.12746101 -0.00754029  0.1360874  -0.03018717  0.03984724 -0.09409562\n",
      " -0.06460703  0.14308035  0.10171673  0.24466932 -0.25955637  0.19143581\n",
      "  0.21683377  0.08735504 -0.22108973 -0.02032516 -0.02765589  0.16266611\n",
      " -0.16048381  0.08584293 -0.03607777 -0.16531628 -0.0666057  -0.07318849\n",
      "  0.1106616  -0.00243276  0.15344919  0.03643313  0.20431331  0.1104352\n",
      "  0.31909138  0.13275817  0.10972384 -0.09700152  0.08658484  0.29698787\n",
      "  0.13693842 -0.23576035  0.02126509 -0.17188917 -0.06938644  0.23882056\n",
      "  0.11119061 -0.01466132  0.21608165 -0.0366245   0.091017    0.11597763\n",
      "  0.13236609  0.10260193 -0.21080319  0.05316481  0.10454369  0.14120001\n",
      " -0.18556048  0.06269514 -0.06572266 -0.05951783 -0.03497775  0.18399573\n",
      "  0.0713119   0.27084658 -0.29881328 -0.03917448 -0.21173158 -0.03915175\n",
      "  0.10361301  0.31706198  0.21578872 -0.05128714  0.10252077 -0.00723583\n",
      " -0.18706741 -0.13515017  0.15986423 -0.09182071 -0.12089945 -0.03694012\n",
      "  0.04223131 -0.04344791  0.11779856 -0.10848002 -0.27553789  0.15711596\n",
      "  0.01538217  0.02898766 -0.22160538  0.02208627 -0.22868092  0.01417258\n",
      "  0.08628086 -0.00603918  0.11264766 -0.03627581  0.07501492  0.0253533\n",
      "  1.          0.          2.          1.          2.          0.\n",
      "  0.        ]\n",
      "Extremely Negative\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "y_test = [] \n",
    "\n",
    "for i in range(len(testdata)):\n",
    "    tok = testdata['tokens'].iloc[i]\n",
    "    has = testdata['HashtagsCount'].iloc[i]\n",
    "    men = testdata['Mentions'].iloc[i]\n",
    "    url = testdata['URLs'].iloc[i]\n",
    "    upc = testdata['UpperCaseWords'].iloc[i]\n",
    "    per = testdata['PunctPeriod'].iloc[i]\n",
    "    exc = testdata['PunctExclamation'].iloc[i]\n",
    "    que = testdata['PunctQuestion'].iloc[i]\n",
    "    row = np.concatenate([tok, has, men, url, upc, per, exc, que])\n",
    "    X_test.append(row)\n",
    "    \n",
    "#     HashtagsCount\tMentions\tURLs\tUpperCaseWords\tPunctPeriod\tPunctExclamation\tPunctQuestion\n",
    "    y_test.append(testdata['Sentiment'].iloc[i])\n",
    "    \n",
    "print(X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking shapes of Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307\n",
      "307\n",
      "41106\n",
      "3795\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))\n",
    "print(len(X_test[0]))\n",
    "print(len(X))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Considering tf-idf\n",
    "- Random forest\n",
    "- Logistic Regression\n",
    "- KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Extremely Negative' 'Extremely Positive' 'Negative' 'Neutral' 'Positive']\n",
      "[0 1 2 3 4]\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation of Random Forest\n",
      "Best parameters set found on development set:\n",
      "{'max_depth': 50, 'n_estimators': 500}\n",
      "Best parameters set found on development set:\n",
      "[0.36742885 0.37873997 0.37873997 0.35429336 0.38165896 0.38798346\n",
      " 0.35587448 0.38068596 0.38968621]\n",
      "Best score: \n",
      "0.3896862077353442\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation of logistic regression\n",
      "Best parameters set found on development set:\n",
      "{'C': 0.01}\n",
      "Best parameters set found on development set:\n",
      "[0.39211871 0.45451228 0.45524203 0.45171491 0.45183654]\n",
      "Best score: \n",
      "0.4552420335684748\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Cross Validation of KNN\n",
      "Best parameters set found on development set:\n",
      "{'n_neighbors': 15}\n",
      "Best parameters set found on development set:\n",
      "[0.33799562 0.3623206  0.36755047 0.38032109 0.38567259]\n",
      "Best score: \n",
      "0.3856725857455607\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 11.1min finished\n"
     ]
    }
   ],
   "source": [
    "# devide data to training data (80%) and validation data (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "scaled_X_test = scaler.fit_transform(X_test)\n",
    "pca = PCA(.95)\n",
    "pca.fit(scaled_X)\n",
    "new_X = pca.transform(scaled_X)\n",
    "X_test = pca.transform(scaled_X_test)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "scaled_y = encoder.transform(y)\n",
    "y_test = encoder.transform(y_test)\n",
    "print(encoder.classes_)\n",
    "print(encoder.transform(encoder.classes_))\n",
    "train, cross, label_train, label_cross = train_test_split(new_X, scaled_y, test_size=0.2, random_state=43)\n",
    "\n",
    "\n",
    "#tuning hyperparameters\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "NB = MultinomialNB()\n",
    "RForest = RandomForestClassifier(random_state=1, n_jobs=-1) #naive bayes\n",
    "logistic = LogisticRegression(random_state=0, solver='saga',n_jobs = 1)\n",
    "knn = KNeighborsClassifier()\n",
    "rf_params = {'n_estimators': [50,200,500],\n",
    "              'max_depth':[10, 30, 50]}\n",
    "nb_params = {'alpha': [0,0.001,0.01,0.1,1,10,100]}\n",
    "log_params = {'C':[0.0001,0.001,0.01,0.1,1]}\n",
    "knn_params = {'n_neighbors':[3,5,7,10,15]}\n",
    "\n",
    "def tune_param(esti, param, alg):\n",
    "    eva = GridSearchCV(estimator=esti, \n",
    "                     param_grid=param, \n",
    "                     cv=5,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy')\n",
    "    eva.fit(cross, label_cross)\n",
    "    print(\"Cross Validation of \"+alg)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(eva.best_params_)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(eva.cv_results_['mean_test_score'])\n",
    "    print(\"Best score: \")\n",
    "    print(eva.best_score_)\n",
    "    print()\n",
    "\n",
    "#tune_param(NB, nb_params, \"Navie Bayes\")\n",
    "tune_param(RForest, rf_params, \"Random Forest\")\n",
    "tune_param(logistic, log_params, \"logistic regression\")\n",
    "tune_param(knn, knn_params, \"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing\n",
    "Considering tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "0.3965744400527009\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.16      0.26       592\n",
      "          1       0.71      0.21      0.33       599\n",
      "          2       0.39      0.36      0.37      1041\n",
      "          3       0.54      0.45      0.49       616\n",
      "          4       0.32      0.67      0.43       947\n",
      "\n",
      "avg / total       0.49      0.40      0.38      3795\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "0.4658761528326746\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.44      0.49       592\n",
      "          1       0.60      0.54      0.57       599\n",
      "          2       0.44      0.34      0.38      1041\n",
      "          3       0.47      0.66      0.55       616\n",
      "          4       0.38      0.45      0.41       947\n",
      "\n",
      "avg / total       0.47      0.47      0.46      3795\n",
      "\n",
      "\n",
      "KNN\n",
      "0.397364953886693\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.37      0.43       592\n",
      "          1       0.52      0.35      0.42       599\n",
      "          2       0.38      0.36      0.37      1041\n",
      "          3       0.39      0.61      0.48       616\n",
      "          4       0.33      0.35      0.34       947\n",
      "\n",
      "avg / total       0.41      0.40      0.40      3795\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "final_RF4 = RandomForestClassifier(max_depth=30,n_estimators=500, random_state=1, n_jobs=-1)\n",
    "final_log4 = LogisticRegression(random_state=0, solver='saga',n_jobs = 1, C=0.1)\n",
    "final_knn4 = KNeighborsClassifier(n_neighbors=15)\n",
    "def test_result(model,name):\n",
    "    model.fit(train, label_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    print(name)\n",
    "    print(accuracy_score(y_test, y_predict))\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print()\n",
    "test_result(final_RF4,\"Random Forest\")\n",
    "test_result(final_log4,\"Logistic Regression\")\n",
    "test_result(final_knn4,\"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General features only\n",
    "Sentiment prediction of tweets based on general features only\n",
    "\n",
    "## Prepare trainingset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0]\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = [] \n",
    "\n",
    "for i in range(len(data)):\n",
    "    has = data['HashtagsCount'].iloc[i]\n",
    "    men = data['Mentions'].iloc[i]\n",
    "    url = data['URLs'].iloc[i]\n",
    "    upc = data['UpperCaseWords'].iloc[i]\n",
    "    per = data['PunctPeriod'].iloc[i]\n",
    "    exc = data['PunctExclamation'].iloc[i]\n",
    "    que = data['PunctQuestion'].iloc[i]\n",
    "    row = np.concatenate([has, men, url, upc, per, exc, que])\n",
    "    X.append(row)\n",
    "    y.append(data['Sentiment'].iloc[i])\n",
    "    \n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 2 0 0]\n",
      "Extremely Negative\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "y_test = [] \n",
    "\n",
    "for i in range(len(testdata)):\n",
    "    has = testdata['HashtagsCount'].iloc[i]\n",
    "    men = testdata['Mentions'].iloc[i]\n",
    "    url = testdata['URLs'].iloc[i]\n",
    "    upc = testdata['UpperCaseWords'].iloc[i]\n",
    "    per = testdata['PunctPeriod'].iloc[i]\n",
    "    exc = testdata['PunctExclamation'].iloc[i]\n",
    "    que = testdata['PunctQuestion'].iloc[i]\n",
    "    row = np.concatenate([has, men, url, upc, per, exc, que])\n",
    "    X_test.append(row)\n",
    "    \n",
    "#     HashtagsCount\tMentions\tURLs\tUpperCaseWords\tPunctPeriod\tPunctExclamation\tPunctQuestion\n",
    "    y_test.append(testdata['Sentiment'].iloc[i])\n",
    "    \n",
    "print(X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check shape of Training and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "41106\n",
      "3795\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))\n",
    "print(len(X_test[0]))\n",
    "print(len(X))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Extremely Negative' 'Extremely Positive' 'Negative' 'Neutral' 'Positive']\n",
      "[0 1 2 3 4]\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:   52.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation of Random Forest\n",
      "Best parameters set found on development set:\n",
      "{'max_depth': 10, 'n_estimators': 200}\n",
      "Best parameters set found on development set:\n",
      "[0.27402092 0.28010216 0.27864267 0.25942593 0.26064218 0.26198005\n",
      " 0.25711506 0.25966918 0.26185843]\n",
      "Best score: \n",
      "0.2801021649233763\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    7.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation of logistic regression\n",
      "Best parameters set found on development set:\n",
      "{'C': 0.1}\n",
      "Best parameters set found on development set:\n",
      "[0.27985892 0.28229141 0.28484554 0.28581854 0.28545366]\n",
      "Best score: \n",
      "0.28581853563609827\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Cross Validation of KNN\n",
      "Best parameters set found on development set:\n",
      "{'n_neighbors': 15}\n",
      "Best parameters set found on development set:\n",
      "[0.21053272 0.22768183 0.22451958 0.2368037  0.25103381]\n",
      "Best score: \n",
      "0.25103381172464123\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    4.2s finished\n"
     ]
    }
   ],
   "source": [
    "# devide data to training data (80%) and validation data (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "scaled_X_test = scaler.fit_transform(X_test)\n",
    "pca = PCA(.95)\n",
    "pca.fit(scaled_X)\n",
    "new_X = pca.transform(scaled_X)\n",
    "X_test = pca.transform(scaled_X_test)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "scaled_y = encoder.transform(y)\n",
    "y_test = encoder.transform(y_test)\n",
    "print(encoder.classes_)\n",
    "print(encoder.transform(encoder.classes_))\n",
    "train, cross, label_train, label_cross = train_test_split(new_X, scaled_y, test_size=0.2, random_state=43)\n",
    "\n",
    "\n",
    "#tuning hyperparameters\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "NB = MultinomialNB()\n",
    "RForest = RandomForestClassifier(random_state=1, n_jobs=-1) #naive bayes\n",
    "logistic = LogisticRegression(random_state=0, solver='saga',n_jobs = 1)\n",
    "knn = KNeighborsClassifier()\n",
    "rf_params = {'n_estimators': [50,200,500],\n",
    "              'max_depth':[10, 30, 50]}\n",
    "nb_params = {'alpha': [0,0.001,0.01,0.1,1,10,100]}\n",
    "log_params = {'C':[0.0001,0.001,0.01,0.1,1]}\n",
    "knn_params = {'n_neighbors':[3,5,7,10,15]}\n",
    "\n",
    "def tune_param(esti, param, alg):\n",
    "    eva = GridSearchCV(estimator=esti, \n",
    "                     param_grid=param, \n",
    "                     cv=5,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy')\n",
    "    eva.fit(cross, label_cross)\n",
    "    print(\"Cross Validation of \"+alg)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(eva.best_params_)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(eva.cv_results_['mean_test_score'])\n",
    "    print(\"Best score: \")\n",
    "    print(eva.best_score_)\n",
    "    print()\n",
    "\n",
    "#tune_param(NB, nb_params, \"Navie Bayes\")\n",
    "tune_param(RForest, rf_params, \"Random Forest\")\n",
    "tune_param(logistic, log_params, \"logistic regression\")\n",
    "tune_param(knn, knn_params, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "0.26455862977602107\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.04      0.07       592\n",
      "          1       0.21      0.08      0.12       599\n",
      "          2       0.30      0.19      0.24      1041\n",
      "          3       0.30      0.29      0.30       616\n",
      "          4       0.25      0.58      0.35       947\n",
      "\n",
      "avg / total       0.26      0.26      0.23      3795\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "0.25691699604743085\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.01      0.02       592\n",
      "          1       0.18      0.02      0.03       599\n",
      "          2       0.00      0.00      0.00      1041\n",
      "          3       0.31      0.18      0.23       616\n",
      "          4       0.25      0.90      0.39       947\n",
      "\n",
      "avg / total       0.24      0.26      0.14      3795\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "0.2545454545454545\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.20      0.13      0.16       592\n",
      "          1       0.22      0.17      0.19       599\n",
      "          2       0.27      0.30      0.28      1041\n",
      "          3       0.29      0.24      0.26       616\n",
      "          4       0.26      0.34      0.29       947\n",
      "\n",
      "avg / total       0.25      0.25      0.25      3795\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "final_RF4 = RandomForestClassifier(max_depth=30,n_estimators=500, random_state=1, n_jobs=-1)\n",
    "final_log4 = LogisticRegression(random_state=0, solver='saga',n_jobs = 1, C=0.1)\n",
    "final_knn4 = KNeighborsClassifier(n_neighbors=15)\n",
    "def test_result(model,name):\n",
    "    model.fit(train, label_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    print(name)\n",
    "    print(accuracy_score(y_test, y_predict))\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print()\n",
    "test_result(final_RF4,\"Random Forest\")\n",
    "test_result(final_log4,\"Logistic Regression\")\n",
    "test_result(final_knn4,\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>HashtagsCount</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>URLs</th>\n",
       "      <th>UpperCaseWords</th>\n",
       "      <th>PunctPeriod</th>\n",
       "      <th>PunctExclamation</th>\n",
       "      <th>PunctQuestion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[-0.045217821723781526, -0.12957120189093985, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0.1090496089309454, 0.11484042098163627, 0.10...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0.18580312211997807, -0.07340790703892708, 0....</td>\n",
       "      <td>[covid19france, covid_19, covid19, coronavirus...</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[0.27922972617670894, -0.03418381605297327, 0....</td>\n",
       "      <td>[covid19, coronavirus, coronavirusfrance, rest...</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As news of the regionÃÂs first confirmed COV...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[0.04783004254568368, 0.059689379995688796, -0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet Sentiment  \\\n",
       "1  advice Talk to your neighbours family to excha...  Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...  Positive   \n",
       "3  My food stock is not the only one which is emp...  Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Negative   \n",
       "5  As news of the regionÃÂs first confirmed COV...  Positive   \n",
       "\n",
       "                                              tokens  \\\n",
       "1  [-0.045217821723781526, -0.12957120189093985, ...   \n",
       "2  [0.1090496089309454, 0.11484042098163627, 0.10...   \n",
       "3  [0.18580312211997807, -0.07340790703892708, 0....   \n",
       "4  [0.27922972617670894, -0.03418381605297327, 0....   \n",
       "5  [0.04783004254568368, 0.059689379995688796, -0...   \n",
       "\n",
       "                                            Hashtags HashtagsCount Mentions  \\\n",
       "1                                                 []           [0]      [0]   \n",
       "2                                                 []           [0]      [0]   \n",
       "3  [covid19france, covid_19, covid19, coronavirus...           [7]      [0]   \n",
       "4  [covid19, coronavirus, coronavirusfrance, rest...           [6]      [0]   \n",
       "5                                                 []           [0]      [1]   \n",
       "\n",
       "  URLs UpperCaseWords PunctPeriod PunctExclamation PunctQuestion  \n",
       "1  [0]            [1]         [0]              [0]           [0]  \n",
       "2  [1]            [1]         [1]              [0]           [0]  \n",
       "3  [1]           [11]         [6]              [0]           [0]  \n",
       "4  [1]            [2]         [7]              [0]           [0]  \n",
       "5  [1]            [1]         [1]              [0]           [0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sent_token</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>HashtagsCount</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>URLs</th>\n",
       "      <th>UpperCaseWords</th>\n",
       "      <th>PunctPeriod</th>\n",
       "      <th>PunctExclamation</th>\n",
       "      <th>PunctQuestion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>['TRENDING: New Yorkers encounter empty superm...</td>\n",
       "      <td>[0.1975125118624419, 0.16406913893297315, -0.0...</td>\n",
       "      <td>[coronavirus]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[\"When I couldn't find hand sanitizer at Fred ...</td>\n",
       "      <td>[0.012639568885788321, 0.09953883814159781, -0...</td>\n",
       "      <td>[amazon, coronavirus]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>['Find out how you can protect yourself and lo...</td>\n",
       "      <td>[-0.01927915937267244, -0.0742998868227005, -0...</td>\n",
       "      <td>[coronavirus]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>['#Panic buying hits #NewYork City as anxious ...</td>\n",
       "      <td>[-0.16155412141233683, 0.04214186384342611, -0...</td>\n",
       "      <td>[panic, newyork, healthcare, bigapple, coronav...</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>['#toiletpaper #dunnypaper #coronavirus #coron...</td>\n",
       "      <td>[0.13522900408133864, -0.1849645283073187, -0....</td>\n",
       "      <td>[toiletpaper, dunnypaper, coronavirus, coronav...</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...  Positive   \n",
       "2  Find out how you can protect yourself and love...  Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...  Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...   Neutral   \n",
       "\n",
       "                                          sent_token  \\\n",
       "0  ['TRENDING: New Yorkers encounter empty superm...   \n",
       "1  [\"When I couldn't find hand sanitizer at Fred ...   \n",
       "2  ['Find out how you can protect yourself and lo...   \n",
       "3  ['#Panic buying hits #NewYork City as anxious ...   \n",
       "4  ['#toiletpaper #dunnypaper #coronavirus #coron...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [0.1975125118624419, 0.16406913893297315, -0.0...   \n",
       "1  [0.012639568885788321, 0.09953883814159781, -0...   \n",
       "2  [-0.01927915937267244, -0.0742998868227005, -0...   \n",
       "3  [-0.16155412141233683, 0.04214186384342611, -0...   \n",
       "4  [0.13522900408133864, -0.1849645283073187, -0....   \n",
       "\n",
       "                                            Hashtags HashtagsCount Mentions  \\\n",
       "0                                      [coronavirus]           [1]      [0]   \n",
       "1                              [amazon, coronavirus]           [2]      [0]   \n",
       "2                                      [coronavirus]           [1]      [0]   \n",
       "3  [panic, newyork, healthcare, bigapple, coronav...          [11]      [0]   \n",
       "4  [toiletpaper, dunnypaper, coronavirus, coronav...          [11]      [0]   \n",
       "\n",
       "  URLs UpperCaseWords PunctPeriod PunctExclamation PunctQuestion  \n",
       "0  [2]            [1]         [2]              [0]           [0]  \n",
       "1  [1]            [2]         [4]              [2]           [2]  \n",
       "2  [0]            [0]         [1]              [0]           [1]  \n",
       "3  [2]            [6]         [2]              [0]           [1]  \n",
       "4  [1]            [0]         [2]              [0]           [0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_norm = data\n",
    "data_norm.loc[data_norm.Sentiment == \"Extremely Negative\", \"Sentiment\"] = \"Negative\"\n",
    "data_norm.loc[data_norm.Sentiment == \"Extremely Positive\", \"Sentiment\"] = \"Positive\"\n",
    "display(data_norm.head())\n",
    "\n",
    "testdata_norm = testdata\n",
    "testdata_norm.loc[testdata_norm.Sentiment == \"Extremely Negative\", \"Sentiment\"] = \"Negative\"\n",
    "testdata_norm.loc[testdata_norm.Sentiment == \"Extremely Positive\", \"Sentiment\"] = \"Positive\"\n",
    "display(testdata_norm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04521782 -0.1295712  -0.03599979 -0.03202558 -0.13110606 -0.0385706\n",
      "  0.06007127 -0.19903495  0.48167129  0.26953838 -0.06201406 -0.1218441\n",
      " -0.13611413  0.12963183 -0.67325786  0.39216037  0.19548158  0.2820546\n",
      " -0.01657487 -0.2635157   0.54904745  0.11781592 -0.13391594  0.15330124\n",
      " -0.01461942 -0.07108847 -0.5629061   0.31965716  0.09751459 -0.02628797\n",
      " -0.19790317 -0.36990661 -0.30469168 -0.36872784 -0.28543786 -0.28384049\n",
      "  0.35927944  0.00246197 -0.04935208  0.29575096  0.02854061 -0.03263297\n",
      "  0.06597441 -0.14169426 -0.29342194 -0.32867927 -0.23560393  0.13801583\n",
      " -0.35742032 -0.11407956 -0.14610504 -0.12540382 -0.05014944 -0.14147434\n",
      " -0.03050272 -0.02898542 -0.09951725 -0.15503716 -0.16825065 -0.12854678\n",
      " -0.17628692 -0.17272498 -0.31981235  0.11415113 -0.0162946   0.37618858\n",
      " -0.54127505  0.42038736  0.02666528  0.2927442   0.03846314  0.01600092\n",
      "  0.63625876  0.11625102 -0.58021663  0.05137449  0.23431435  0.44318428\n",
      "  0.24013518 -0.03901059  0.23346466 -0.1976378   0.10083642  0.20689596\n",
      "  0.01789333 -0.23957685 -0.46436597  0.17938392 -0.08316787  0.27076973\n",
      "  0.27434829  0.24748793 -0.63954813 -0.88018598  0.09924747  0.01122061\n",
      " -0.0195714   0.06311035  0.19784781 -0.26661193 -0.23380754 -0.49815954\n",
      " -0.11647066  0.39479216  0.09278085 -0.47483247 -0.0633423  -0.07260747\n",
      "  0.23217077  0.00867231 -0.08341036 -0.42505055 -0.30743019 -0.53680941\n",
      "  0.15305104  0.09011814 -0.10241419 -0.16243587  0.86218051  0.46957517\n",
      " -0.49070517  0.39900377 -0.31497765  0.66911347  0.28296031 -0.10182241\n",
      " -0.23099909 -0.45089992  0.29533373  0.43318469 -0.03588073 -0.63643939\n",
      " -0.40879061 -0.14733592 -0.09345967 -0.23306594 -0.16821778  0.15804167\n",
      " -0.08156015  0.12693961  0.1836596  -0.10422981 -0.0388713   0.32125224\n",
      "  0.25665321  0.31603581 -0.1081661  -0.39758499 -0.47277077  0.23657194\n",
      "  0.74027125  0.44731292 -0.3043724   0.11344478  0.09686933 -0.49851309\n",
      " -0.25370177 -0.30852374 -0.26901762 -0.09308902 -0.40207565  0.46156818\n",
      "  0.03970939 -0.08548015 -0.02453746 -0.00720868 -0.07671003 -0.51956227\n",
      " -0.06296346 -0.3161487  -0.25838603  0.1111845  -0.09656814 -0.67387173\n",
      " -0.58063486 -0.2565327   0.47059963 -0.16118343 -0.46456896  0.24986595\n",
      " -0.36695098 -0.24112469  0.05514729  0.11592064  0.51382068 -0.45615761\n",
      " -0.1694997   0.40589321 -0.07978148  0.39654329 -0.28126597 -0.27520746\n",
      " -0.16621179  0.15907976 -0.16161929  0.23605652 -0.22176851 -0.17132468\n",
      " -0.13831694 -0.60460293  0.0111721   0.75983437  0.05226977 -0.28949825\n",
      " -0.10208201 -0.05890625 -0.27025153 -0.0496062   0.11205503 -0.10305065\n",
      "  0.01735331 -0.05367871 -0.2628164  -0.24064734 -0.15607197  0.39957548\n",
      "  0.27444684  0.18559163 -0.75526157  0.07229714  0.08276545  0.25939163\n",
      " -0.17208243  0.00471817  0.2903035   0.03798967 -0.14776344  0.18684402\n",
      " -0.17721193  0.0035297   0.23957992 -0.07856107 -0.13986394 -0.23984607\n",
      "  0.26422493 -0.45635445  0.28708533 -0.28688613 -0.44610652 -0.02673019\n",
      "  0.24526417 -0.0660641  -0.0285436  -0.41011817  0.49996792  0.31485855\n",
      "  0.12440597  0.16378731  0.04679703 -0.35473429 -0.12670339  0.13711185\n",
      "  0.21139856  0.13698987  0.25280809  0.05945187  0.27020388 -0.34051124\n",
      " -0.43940976  0.10658127  0.07468064 -0.23133763 -0.6542546  -0.13603362\n",
      "  0.03741821  0.7038952  -0.1550848   0.0192078  -0.46533787 -0.23540067\n",
      "  0.29035959  0.36398034  0.4583884  -0.05488196  0.11860827 -0.35857842\n",
      "  0.16020646 -0.59284296  0.19115831 -0.09873478  0.15718342 -0.03712953\n",
      "  0.19221001  0.16846936 -0.17116538  0.01517593 -0.23723344  0.37393441\n",
      "  0.32823932  0.45325934 -0.02711843  0.45807185 -0.27584718 -0.20166472\n",
      "  0.21363927 -0.0688255  -0.21754054 -0.10169286 -0.18683029 -0.30936813\n",
      "  0.          0.          0.          1.          0.          0.\n",
      "  0.        ]\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = [] \n",
    "\n",
    "for i in range(len(data_norm)):\n",
    "    tok = data_norm['tokens'].iloc[i]\n",
    "    has = data_norm['HashtagsCount'].iloc[i]\n",
    "    men = data_norm['Mentions'].iloc[i]\n",
    "    url = data_norm['URLs'].iloc[i]\n",
    "    upc = data_norm['UpperCaseWords'].iloc[i]\n",
    "    per = data_norm['PunctPeriod'].iloc[i]\n",
    "    exc = data_norm['PunctExclamation'].iloc[i]\n",
    "    que = data_norm['PunctQuestion'].iloc[i]\n",
    "    row = np.concatenate([tok, has, men, url, upc, per, exc, que])\n",
    "    X.append(row)\n",
    "    y.append(data_norm['Sentiment'].iloc[i])\n",
    "    \n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19751251  0.16406914 -0.08077186  0.24873493 -0.2236984  -0.02667778\n",
      "  0.03970695 -0.09416564  0.15056637  0.49147954 -0.18753959 -0.05954439\n",
      " -0.076067    0.14337692 -0.5960996   0.31317267 -0.04269212  0.1886496\n",
      "  0.0173678  -0.46386353  0.24728703  0.14169849 -0.01813464 -0.04459166\n",
      " -0.05791089  0.01703206 -0.01445083  0.08936925  0.21961409 -0.13089258\n",
      " -0.02144561  0.07451568 -0.25947715 -0.14026356 -0.05995768  0.08417742\n",
      "  0.14595186 -0.1311665   0.17901262  0.10232114  0.14483824  0.17584943\n",
      "  0.21348609  0.03781949 -0.2827316  -0.25911544 -0.06880158  0.05365289\n",
      " -0.17094703  0.05027327  0.07664106  0.14587476 -0.19888585 -0.12783226\n",
      "  0.12424986 -0.19563302  0.00774756 -0.18017244  0.2547958  -0.08037268\n",
      " -0.10337515  0.01341996 -0.24838854  0.14635911 -0.30849758  0.02157698\n",
      " -0.10823765  0.15720974  0.10605738  0.12602015 -0.07108232  0.17590671\n",
      "  0.21743952  0.20612701 -0.39072273  0.02816121  0.13147974  0.32887982\n",
      "  0.08348509  0.17350902  0.0487698  -0.27896093  0.17237388 -0.13369453\n",
      "  0.13272205  0.15422608 -0.24198799  0.19205977  0.10457    -0.03660005\n",
      "  0.13339865 -0.09125687 -0.29298055  0.04625358 -0.04584474 -0.05216529\n",
      "  0.16911994  0.17774936  0.09093266 -0.0023599   0.15440793 -0.06085693\n",
      " -0.05258558 -0.0653673   0.23149419 -0.42805516 -0.0426106  -0.03771289\n",
      " -0.02413051 -0.08053472  0.08541561 -0.06058884  0.00738425 -0.2143093\n",
      "  0.11120974 -0.25393374  0.06485401 -0.06623021  0.26758571  0.27653802\n",
      " -0.05931437 -0.14276723 -0.22198145  0.09963645 -0.06523434  0.0688066\n",
      " -0.09492301 -0.09303237  0.08248964  0.22418382 -0.09815485 -0.14735984\n",
      "  0.00652888  0.16382311  0.03793776  0.12631357 -0.08141186  0.16454645\n",
      " -0.34697858  0.0804178   0.27003656 -0.47439067 -0.12682074  0.0624681\n",
      "  0.15987233 -0.10732967  0.03226423 -0.18852333 -0.2734011  -0.10346964\n",
      "  0.06943845  0.00211249 -0.20207323  0.12108171 -0.09144524 -0.10757578\n",
      " -0.24550526 -0.45698932 -0.23086602  0.03548447 -0.14320574  0.0230671\n",
      " -0.06327092 -0.24045715 -0.02304434 -0.03270765  0.3242679  -0.12888756\n",
      " -0.01969518 -0.06863271 -0.19618603 -0.18552941 -0.2321452  -0.22535587\n",
      " -0.20863198  0.09735194  0.19544132 -0.09523545  0.01789872  0.10417852\n",
      " -0.35836347 -0.31362822  0.04743513 -0.32232821  0.11672675 -0.26700878\n",
      " -0.09853852  0.42764071  0.08689826  0.38295452 -0.00469036 -0.06439551\n",
      " -0.07377241  0.13441635 -0.24590511  0.17260523  0.14328456  0.28830447\n",
      " -0.21900562 -0.0446525   0.04514054  0.12026611 -0.21141133 -0.28553823\n",
      "  0.12746101 -0.00754029  0.1360874  -0.03018717  0.03984724 -0.09409562\n",
      " -0.06460703  0.14308035  0.10171673  0.24466932 -0.25955637  0.19143581\n",
      "  0.21683377  0.08735504 -0.22108973 -0.02032516 -0.02765589  0.16266611\n",
      " -0.16048381  0.08584293 -0.03607777 -0.16531628 -0.0666057  -0.07318849\n",
      "  0.1106616  -0.00243276  0.15344919  0.03643313  0.20431331  0.1104352\n",
      "  0.31909138  0.13275817  0.10972384 -0.09700152  0.08658484  0.29698787\n",
      "  0.13693842 -0.23576035  0.02126509 -0.17188917 -0.06938644  0.23882056\n",
      "  0.11119061 -0.01466132  0.21608165 -0.0366245   0.091017    0.11597763\n",
      "  0.13236609  0.10260193 -0.21080319  0.05316481  0.10454369  0.14120001\n",
      " -0.18556048  0.06269514 -0.06572266 -0.05951783 -0.03497775  0.18399573\n",
      "  0.0713119   0.27084658 -0.29881328 -0.03917448 -0.21173158 -0.03915175\n",
      "  0.10361301  0.31706198  0.21578872 -0.05128714  0.10252077 -0.00723583\n",
      " -0.18706741 -0.13515017  0.15986423 -0.09182071 -0.12089945 -0.03694012\n",
      "  0.04223131 -0.04344791  0.11779856 -0.10848002 -0.27553789  0.15711596\n",
      "  0.01538217  0.02898766 -0.22160538  0.02208627 -0.22868092  0.01417258\n",
      "  0.08628086 -0.00603918  0.11264766 -0.03627581  0.07501492  0.0253533\n",
      "  1.          0.          2.          1.          2.          0.\n",
      "  0.        ]\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "y_test = [] \n",
    "\n",
    "for i in range(len(testdata_norm)):\n",
    "    tok = testdata_norm['tokens'].iloc[i]\n",
    "    has = testdata_norm['HashtagsCount'].iloc[i]\n",
    "    men = testdata_norm['Mentions'].iloc[i]\n",
    "    url = testdata_norm['URLs'].iloc[i]\n",
    "    upc = testdata_norm['UpperCaseWords'].iloc[i]\n",
    "    per = testdata_norm['PunctPeriod'].iloc[i]\n",
    "    exc = testdata_norm['PunctExclamation'].iloc[i]\n",
    "    que = testdata_norm['PunctQuestion'].iloc[i]\n",
    "    row = np.concatenate([tok, has, men, url, upc, per, exc, que])\n",
    "    X_test.append(row)\n",
    "    \n",
    "#     HashtagsCount\tMentions\tURLs\tUpperCaseWords\tPunctPeriod\tPunctExclamation\tPunctQuestion\n",
    "    y_test.append(testdata_norm['Sentiment'].iloc[i])\n",
    "    \n",
    "print(X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307\n",
      "307\n",
      "41106\n",
      "3795\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))\n",
    "print(len(X_test[0]))\n",
    "print(len(X))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Neutral' 'Positive']\n",
      "[0 1 2]\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation of Random Forest\n",
      "Best parameters set found on development set:\n",
      "{'max_depth': 50, 'n_estimators': 500}\n",
      "Best parameters set found on development set:\n",
      "[0.57115057 0.58197519 0.58659694 0.5649477  0.59365118 0.59681343\n",
      " 0.5649477  0.59243493 0.59754318]\n",
      "Best score: \n",
      "0.5975431768426174\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation of logistic regression\n",
      "Best parameters set found on development set:\n",
      "{'C': 0.01}\n",
      "Best parameters set found on development set:\n",
      "[0.61092192 0.65166626 0.65872051 0.65774751 0.65799076]\n",
      "Best score: \n",
      "0.6587205059596205\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Cross Validation of KNN\n",
      "Best parameters set found on development set:\n",
      "{'n_neighbors': 15}\n",
      "Best parameters set found on development set:\n",
      "[0.54183897 0.55631233 0.57650207 0.58270494 0.59413768]\n",
      "Best score: \n",
      "0.5941376793967404\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 11.0min finished\n"
     ]
    }
   ],
   "source": [
    "# devide data to training data (80%) and validation data (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "scaled_X_test = scaler.fit_transform(X_test)\n",
    "pca = PCA(.95)\n",
    "pca.fit(scaled_X)\n",
    "new_X = pca.transform(scaled_X)\n",
    "X_test = pca.transform(scaled_X_test)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "scaled_y = encoder.transform(y)\n",
    "y_test = encoder.transform(y_test)\n",
    "print(encoder.classes_)\n",
    "print(encoder.transform(encoder.classes_))\n",
    "train, cross, label_train, label_cross = train_test_split(new_X, scaled_y, test_size=0.2, random_state=43)\n",
    "\n",
    "\n",
    "#tuning hyperparameters\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "NB = MultinomialNB()\n",
    "RForest = RandomForestClassifier(random_state=1, n_jobs=-1) #naive bayes\n",
    "logistic = LogisticRegression(random_state=0, solver='saga',n_jobs = 1)\n",
    "knn = KNeighborsClassifier()\n",
    "rf_params = {'n_estimators': [50,200,500],\n",
    "              'max_depth':[10, 30, 50]}\n",
    "nb_params = {'alpha': [0,0.001,0.01,0.1,1,10,100]}\n",
    "log_params = {'C':[0.0001,0.001,0.01,0.1,1]}\n",
    "knn_params = {'n_neighbors':[3,5,7,10,15]}\n",
    "\n",
    "def tune_param(esti, param, alg):\n",
    "    eva = GridSearchCV(estimator=esti, \n",
    "                     param_grid=param, \n",
    "                     cv=5,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy')\n",
    "    eva.fit(cross, label_cross)\n",
    "    print(\"Cross Validation of \"+alg)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(eva.best_params_)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(eva.cv_results_['mean_test_score'])\n",
    "    print(\"Best score: \")\n",
    "    print(eva.best_score_)\n",
    "    print()\n",
    "\n",
    "#tune_param(NB, nb_params, \"Navie Bayes\")\n",
    "tune_param(RForest, rf_params, \"Random Forest\")\n",
    "tune_param(logistic, log_params, \"logistic regression\")\n",
    "tune_param(knn, knn_params, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "0.6184453227931489\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.56      0.63      1633\n",
      "          1       0.72      0.25      0.37       616\n",
      "          2       0.56      0.82      0.66      1546\n",
      "\n",
      "avg / total       0.65      0.62      0.60      3795\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "0.6693017127799736\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.64      0.68      1633\n",
      "          1       0.57      0.54      0.56       616\n",
      "          2       0.65      0.75      0.70      1546\n",
      "\n",
      "avg / total       0.67      0.67      0.67      3795\n",
      "\n",
      "\n",
      "KNN\n",
      "0.6050065876152833\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.62      0.64      1633\n",
      "          1       0.46      0.54      0.49       616\n",
      "          2       0.62      0.62      0.62      1546\n",
      "\n",
      "avg / total       0.61      0.61      0.61      3795\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "final_RF4 = RandomForestClassifier(max_depth=30,n_estimators=500, random_state=1, n_jobs=-1)\n",
    "final_log4 = LogisticRegression(random_state=0, solver='saga',n_jobs = 1, C=0.1)\n",
    "final_knn4 = KNeighborsClassifier(n_neighbors=15)\n",
    "def test_result(model,name):\n",
    "    model.fit(train, label_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    print(name)\n",
    "    print(accuracy_score(y_test, y_predict))\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print()\n",
    "test_result(final_RF4,\"Random Forest\")\n",
    "test_result(final_log4,\"Logistic Regression\")\n",
    "test_result(final_knn4,\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0]\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = [] \n",
    "\n",
    "for i in range(len(data_norm)):\n",
    "    has = data_norm['HashtagsCount'].iloc[i]\n",
    "    men = data_norm['Mentions'].iloc[i]\n",
    "    url = data_norm['URLs'].iloc[i]\n",
    "    upc = data_norm['UpperCaseWords'].iloc[i]\n",
    "    per = data_norm['PunctPeriod'].iloc[i]\n",
    "    exc = data_norm['PunctExclamation'].iloc[i]\n",
    "    que = data_norm['PunctQuestion'].iloc[i]\n",
    "    row = np.concatenate([has, men, url, upc, per, exc, que])\n",
    "    X.append(row)\n",
    "    y.append(data_norm['Sentiment'].iloc[i])\n",
    "    \n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 2 0 0]\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "y_test = [] \n",
    "\n",
    "for i in range(len(testdata_norm)):\n",
    "    has = testdata_norm['HashtagsCount'].iloc[i]\n",
    "    men = testdata_norm['Mentions'].iloc[i]\n",
    "    url = testdata_norm['URLs'].iloc[i]\n",
    "    upc = testdata_norm['UpperCaseWords'].iloc[i]\n",
    "    per = testdata_norm['PunctPeriod'].iloc[i]\n",
    "    exc = testdata_norm['PunctExclamation'].iloc[i]\n",
    "    que = testdata_norm['PunctQuestion'].iloc[i]\n",
    "    row = np.concatenate([has, men, url, upc, per, exc, que])\n",
    "    X_test.append(row)\n",
    "    \n",
    "#     HashtagsCount\tMentions\tURLs\tUpperCaseWords\tPunctPeriod\tPunctExclamation\tPunctQuestion\n",
    "    y_test.append(testdata_norm['Sentiment'].iloc[i])\n",
    "    \n",
    "print(X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "41106\n",
      "3795\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))\n",
    "print(len(X_test[0]))\n",
    "print(len(X))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Neutral' 'Positive']\n",
      "[0 1 2]\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:   48.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation of Random Forest\n",
      "Best parameters set found on development set:\n",
      "{'max_depth': 10, 'n_estimators': 500}\n",
      "Best parameters set found on development set:\n",
      "[0.43006568 0.43298468 0.43529555 0.41778156 0.41997081 0.42483581\n",
      " 0.41875456 0.42264656 0.42422768]\n",
      "Best score: \n",
      "0.4352955485283386\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation of logistic regression\n",
      "Best parameters set found on development set:\n",
      "{'C': 0.01}\n",
      "Best parameters set found on development set:\n",
      "[0.4374848  0.43772805 0.44003892 0.44003892 0.44003892]\n",
      "Best score: \n",
      "0.44003891997081\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Cross Validation of KNN\n",
      "Best parameters set found on development set:\n",
      "{'n_neighbors': 15}\n",
      "Best parameters set found on development set:\n",
      "[0.3979567  0.38774021 0.4003892  0.40452445 0.41838969]\n",
      "Best score: \n",
      "0.41838968620773537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    3.9s finished\n"
     ]
    }
   ],
   "source": [
    "# devide data to training data (80%) and validation data (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "scaled_X_test = scaler.fit_transform(X_test)\n",
    "pca = PCA(.95)\n",
    "pca.fit(scaled_X)\n",
    "new_X = pca.transform(scaled_X)\n",
    "X_test = pca.transform(scaled_X_test)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "scaled_y = encoder.transform(y)\n",
    "y_test = encoder.transform(y_test)\n",
    "print(encoder.classes_)\n",
    "print(encoder.transform(encoder.classes_))\n",
    "train, cross, label_train, label_cross = train_test_split(new_X, scaled_y, test_size=0.2, random_state=43)\n",
    "\n",
    "\n",
    "#tuning hyperparameters\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "NB = MultinomialNB()\n",
    "RForest = RandomForestClassifier(random_state=1, n_jobs=-1) #naive bayes\n",
    "logistic = LogisticRegression(random_state=0, solver='saga',n_jobs = 1)\n",
    "knn = KNeighborsClassifier()\n",
    "rf_params = {'n_estimators': [50,200,500],\n",
    "              'max_depth':[10, 30, 50]}\n",
    "nb_params = {'alpha': [0,0.001,0.01,0.1,1,10,100]}\n",
    "log_params = {'C':[0.0001,0.001,0.01,0.1,1]}\n",
    "knn_params = {'n_neighbors':[3,5,7,10,15]}\n",
    "\n",
    "def tune_param(esti, param, alg):\n",
    "    eva = GridSearchCV(estimator=esti, \n",
    "                     param_grid=param, \n",
    "                     cv=5,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy')\n",
    "    eva.fit(cross, label_cross)\n",
    "    print(\"Cross Validation of \"+alg)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(eva.best_params_)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(eva.cv_results_['mean_test_score'])\n",
    "    print(\"Best score: \")\n",
    "    print(eva.best_score_)\n",
    "    print()\n",
    "\n",
    "#tune_param(NB, nb_params, \"Navie Bayes\")\n",
    "tune_param(RForest, rf_params, \"Random Forest\")\n",
    "tune_param(logistic, log_params, \"logistic regression\")\n",
    "tune_param(knn, knn_params, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "0.4276679841897233\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.35      0.39      1633\n",
      "          1       0.34      0.09      0.14       616\n",
      "          2       0.42      0.64      0.51      1546\n",
      "\n",
      "avg / total       0.42      0.43      0.40      3795\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "0.40869565217391307\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.00      0.01      1633\n",
      "          1       0.40      0.04      0.07       616\n",
      "          2       0.41      0.98      0.58      1546\n",
      "\n",
      "avg / total       0.46      0.41      0.25      3795\n",
      "\n",
      "\n",
      "KNN\n",
      "0.42845849802371544\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.48      0.46      1633\n",
      "          1       0.38      0.12      0.18       616\n",
      "          2       0.43      0.50      0.46      1546\n",
      "\n",
      "avg / total       0.42      0.43      0.41      3795\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "final_RF4 = RandomForestClassifier(max_depth=30,n_estimators=500, random_state=1, n_jobs=-1)\n",
    "final_log4 = LogisticRegression(random_state=0, solver='saga',n_jobs = 1, C=0.1)\n",
    "final_knn4 = KNeighborsClassifier(n_neighbors=15)\n",
    "def test_result(model,name):\n",
    "    model.fit(train, label_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    print(name)\n",
    "    print(accuracy_score(y_test, y_predict))\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print()\n",
    "test_result(final_RF4,\"Random Forest\")\n",
    "test_result(final_log4,\"Logistic Regression\")\n",
    "test_result(final_knn4,\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
